{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\IMOE001\\AppData\\Local\\Temp\\ipykernel_13324\\163558748.py:2: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as pyp\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#phishing_url_df = pd.read_csv(\"../Input/data_phishing_37175.csv\")\n",
    "col_names = [\"index\", \"url\"]\n",
    "#phishing_url_df.columns = col_names\n",
    "\n",
    "#phishing_url_df = list(phishing_url_df[\"url\"])\n",
    "#first_50_phishing_url_df = phishing_url_df[:] \n",
    "#print(first_50_phishing_url_df)\n",
    "\n",
    "legitimate_url_df = pd.read_csv(\"../Input/data_legitimate_36400.csv\")\n",
    "legitimate_url_df.columns = col_names\n",
    "legitimate_urls = list(legitimate_url_df[\"url\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import domain_parser\n",
    "\n",
    "#parsed_urls = domain_parser.parse(first_50_phishing_url_df, \"phishing\")\n",
    "#print(parsed_urls)\n",
    "\n",
    "parsed_urls = domain_parser.parse(legitimate_urls, \"legitimate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataPreProcessing import filter_brands\n",
    "from dataPreProcessing import filter_keywords\n",
    "from dataPreProcessing import subtract_lists\n",
    "from dataPreProcessing import filter_randoms\n",
    "from dataPreProcessing import filter_composes\n",
    "from dataPreProcessing import brand_names\n",
    "from dataPreProcessing import keywords\n",
    "from dataPreProcessing import filter_maliciouses\n",
    "\n",
    "for parsed_url in parsed_urls:\n",
    "    parsed_url[\"words_raw_for_filters\"] = list(parsed_url[\"words_raw\"]) # remove from all dicts at the end\n",
    "    \n",
    "    # brands\n",
    "    parsed_url[\"brand_words\"] = filter_brands(parsed_url[\"words_raw_for_filters\"])\n",
    "    parsed_url[\"words_raw_for_filters\"] = subtract_lists(parsed_url[\"words_raw_for_filters\"], parsed_url[\"brand_words\"])\n",
    "\n",
    "    # keywords\n",
    "    parsed_url[\"keyword_words\"] = filter_keywords(parsed_url[\"words_raw_for_filters\"])\n",
    "    parsed_url[\"words_raw_for_filters\"] = subtract_lists(parsed_url[\"words_raw_for_filters\"], parsed_url[\"keyword_words\"])\n",
    "\n",
    "    # randoms\n",
    "    parsed_url[\"random_words\"] = filter_randoms(parsed_url[\"words_raw_for_filters\"])\n",
    "    parsed_url[\"words_raw_for_filters\"] = subtract_lists(parsed_url[\"words_raw_for_filters\"], parsed_url[\"random_words\"])\n",
    "\n",
    "    # filter word less then 7 letters\n",
    "    parsed_url[\"word_list\"] = []\n",
    "    for word in list(parsed_url[\"words_raw_for_filters\"]):\n",
    "        if len(word) <= 7:\n",
    "            parsed_url[\"word_list\"].append(word)\n",
    "            parsed_url[\"words_raw_for_filters\"].remove(word)\n",
    "    \n",
    "    # try to decompose the rest\n",
    "    parsed_url[\"word_composed\"] = []\n",
    "    for word, composed in filter_composes(parsed_url[\"words_raw_for_filters\"]):\n",
    "        for internal_word in composed:\n",
    "            if word in brand_names():\n",
    "                parsed_url[\"brand_words\"].append(internal_word)\n",
    "            \n",
    "            elif word in keywords():\n",
    "                parsed_url[\"keyword_words\"].append(internal_word)\n",
    "            \n",
    "            else:\n",
    "                parsed_url[\"word_list\"].append(internal_word)\n",
    "        \n",
    "        parsed_url[\"word_composed\"].append(word)\n",
    "        parsed_url[\"words_raw_for_filters\"].remove(word)\n",
    "\n",
    "    \n",
    "    # the rest is non brands, non keywords, non randoms, with len more then 7 chars\n",
    "    # and no composed.\n",
    "    for word in list(parsed_url[\"words_raw_for_filters\"]):\n",
    "        parsed_url[\"word_list\"].append(word)\n",
    "        parsed_url[\"words_raw_for_filters\"].remove(word)\n",
    "    \n",
    "\n",
    "    # check the rest for maliciousness\n",
    "    parsed_url[\"word_malicious\"] = [mal_word for (mal_word, _, _) in filter_maliciouses(parsed_url[\"word_list\"])]\n",
    "    parsed_url[\"word_list\"] = subtract_lists(parsed_url[\"word_list\"], list(set(parsed_url[\"word_malicious\"])))\n",
    "\n",
    "\n",
    "    #now can remove the helper list 'words_raw_for_filters'\n",
    "    parsed_url.pop('words_raw_for_filters')\n",
    "\n",
    "\n",
    "\n",
    "#print(parsed_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36400\n"
     ]
    }
   ],
   "source": [
    "print(len(parsed_urls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "#with open(\"../Input/preprocess_phishing_37175.json\", \"w\") as f:\n",
    "#    json.dump(parsed_urls, f)\n",
    "\n",
    "with open(\"../Input/preprocess_legitimate_36400.json\", \"w\") as f:\n",
    "    json.dump(parsed_urls, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36400\n",
      "37175\n"
     ]
    }
   ],
   "source": [
    "with open(\"../Input/preprocess_legitimate_36400.json\", \"r\") as f:\n",
    "    l = json.loads(f.read())\n",
    "print(len(l))\n",
    "\n",
    "with open(\"../Input/preprocess_phishing_37175.json\", \"r\") as f:\n",
    "    l2 = json.loads(f.read())\n",
    "print(len(l2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
